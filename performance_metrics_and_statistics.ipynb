{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796c2a7-c19d-46e3-a61e-ca2a726f6bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bab18d-8362-46dc-a219-001924cefc22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inter-rater reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c66679-a5e2-4775-a13d-62953bd41c02",
   "metadata": {},
   "source": [
    "<u>Percent agreement</u> was used to assess inter-rater reliability for index metastasis date (used in timing classification). Percent agreement was calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\text{Percent Agreement} = \\left( \\frac{\\text{Total number of labels} - \\text{Number of mismatched labels}}{\\text{Total number of labels}} \\right) \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d09669-a3b0-494b-8bce-59f814a1e29d",
   "metadata": {},
   "source": [
    "<u>Cohen's kappa</u> was used to assess inter-rater reliability for volume classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672189c1-d669-4631-85df-f7f509acd122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ae26d-7be6-4b1f-ae67-786967605357",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Arrays of volume classification labels for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a43fe-0349-4b3a-9404-653328ba1528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arrays of volume labels for reference\n",
    "none_pd_pred = np.array(['L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H'])\n",
    "low_pd_pred = np.array(['L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L'])\n",
    "high_pd_pred = np.array(['L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L'])\n",
    "high_rbp_pd_pred = np.array(['L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L'])\n",
    "gold_labels = np.array(['L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'L', 'H', 'H', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'L', 'L', 'H', 'L', 'L', 'L', 'L', 'H', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H', 'H', 'L', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'H', 'H', 'H', 'L', 'L', 'L', 'H'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a210e5-baa4-4597-9e73-da3065758015",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accuracy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a14c5-1d94-4185-ad57-aff5900c21de",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Accuracy value and 95% confidence interval__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abadf23-9af0-42df-b22c-7498fd953538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accuracy_score() was used to calculate accuracy, with parameters gold_labels, pred_labels\n",
    "# gold_labels is an array containing the gold standard labels\n",
    "# pred_labels is an array containing the experimental labels, in the same order as gold_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function for 10,000 bootstrap iterations to generate 95% confidence intervals\n",
    "def bootstrap_accuracy(gold_labels, pred_labels):\n",
    "\n",
    "    n_iterations = 10000  # Number of bootstrap resamples\n",
    "    n_samples = len(gold_labels)\n",
    "    bootstrapped_accuracies = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)  # Resample indices\n",
    "        bootstrapped_accuracy = compute_accuracy(gold_labels[indices], pred_labels[indices])\n",
    "        bootstrapped_accuracies.append(bootstrapped_accuracy)\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    lower = np.percentile(bootstrapped_accuracies, 2.5)\n",
    "    upper = np.percentile(bootstrapped_accuracies, 97.5)\n",
    "    \n",
    "    return(round(lower, 4), round(upper, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f5b8b-f56f-454d-ad2c-787e7bf3b1c6",
   "metadata": {},
   "source": [
    "__Testing statistical significance:__\n",
    "\n",
    "We evaluated for statistically significant differences in accuracy between experiments of four prompt decomposition levels: none, low, high, and high with RBP.\n",
    "\n",
    "To adjust for multiple pairwise comparisons and control the risk of Type I error, we applied both the Holm-Bonferroni correction and the more conservative Bonferroni correction.\n",
    "\n",
    "We used alpha = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777030fb-90f3-458f-8e9a-76d9fecb044d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace with your actual gold labels and predicted labels\n",
    "predictions = { \n",
    " \"None\": none_pd_pred,  \n",
    " \"Low\": low_pd_pred,\n",
    " \"High\": high_pd_pred,\n",
    " \"High with RBP\": high_rbp_pd_pred\n",
    "}\n",
    "\n",
    "# Generate all possible pairs of prompts for comparison\n",
    "import itertools\n",
    "prompt_names = list(predictions.keys())\n",
    "pairwise_comparisons = list(itertools.combinations(prompt_names, 2))\n",
    "\n",
    "alpha = 0.05\n",
    "bonferroni_corrected_alpha = alpha / len(pairwise_comparisons)\n",
    "\n",
    "print(pairwise_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc64a47-c4f5-4e6f-b95b-f043dba0e2d4",
   "metadata": {},
   "source": [
    "__McNemar's test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d83c2-fd24-4350-80a7-93fa2a99d803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "p_values = []\n",
    "comparisons = []\n",
    "\n",
    "for p1, p2 in pairwise_comparisons:\n",
    "    pred1 = predictions[p1]\n",
    "    pred2 = predictions[p2]\n",
    "\n",
    "    # Construct contingency table\n",
    "    a = np.sum((pred1 == gold_labels) & (pred2 == gold_labels))\n",
    "    b = np.sum((pred1 == gold_labels) & (pred2 != gold_labels))\n",
    "    c = np.sum((pred1 != gold_labels) & (pred2 == gold_labels))\n",
    "    d = np.sum((pred1 != gold_labels) & (pred2 != gold_labels))\n",
    "\n",
    "    table = [[a, b], [c, d]]\n",
    "\n",
    "    # Perform McNemar's test\n",
    "    result = mcnemar(table, exact = True if (b < 5 or c < 5) else False)\n",
    "        \n",
    "    # Store results\n",
    "    p_values.append(result.pvalue)\n",
    "    comparisons.append((p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e798f0-a532-4ac8-8bd7-a37a9392bbe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Holm-Bonferroni Correction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07a378-7d46-43a4-90fc-f9e71a479437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting p value and pairwise comparisons for Holm-Bonferroni correction\n",
    "m = len(p_values)  # Number of tests\n",
    "sorted_indices = np.argsort(p_values)  # Sort indices by p-value\n",
    "sorted_p_values = (np.array(p_values)[sorted_indices]).tolist()\n",
    "sorted_comparisons = (np.array(comparisons)[sorted_indices]).tolist()\n",
    "\n",
    "# Holm-Bonferroni stepwise correction\n",
    "significant = []\n",
    "alphas = []\n",
    "for i, (p, comp) in enumerate(zip(sorted_p_values, sorted_comparisons)):\n",
    "    alpha_adjusted = alpha / (m - i)  # Adjusted significance level\n",
    "    if p < alpha_adjusted:\n",
    "        significant.append(comp)\n",
    "        alphas.append(alpha_adjusted)\n",
    "    else:\n",
    "        break  # Stop if one test fails (all larger p-values are non-significant)\n",
    "\n",
    "# Print results\n",
    "for (p1, p2), p in zip(comparisons, p_values):\n",
    "    print(f\"Comparison: {p1} vs {p2}\")\n",
    "    print(f\"p-value: {p:.5f}\")\n",
    "\n",
    "    if list((p1, p2)) in significant:\n",
    "        print(\"Significant difference after Holm-Bonferroni correction.\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No significant difference after Holm-Bonferroni correction.\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f2b8b-f776-46bc-bb95-206c72f25d94",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Bonferroni Correction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d6600-05be-4828-be37-c18f0612ed80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p1, p2 in pairwise_comparisons:\n",
    "    pred1 = predictions[p1]\n",
    "    pred2 = predictions[p2]\n",
    "\n",
    "    # Construct contingency table\n",
    "    a = np.sum((pred1 == gold_labels) & (pred2 == gold_labels))\n",
    "    b = np.sum((pred1 == gold_labels) & (pred2 != gold_labels))\n",
    "    c = np.sum((pred1 != gold_labels) & (pred2 == gold_labels))\n",
    "    d = np.sum((pred1 != gold_labels) & (pred2 != gold_labels))\n",
    "\n",
    "    table = [[a, b], [c, d]]\n",
    "    print(table)\n",
    "\n",
    "    # Perform McNemar's test\n",
    "    result = mcnemar(table, exact = True if (b < 5 or c < 5) else False)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Comparison: {p1} vs {p2}\")\n",
    "    print(f\"Contingency Table: {table}\")\n",
    "    print(f\"p-value: {result.pvalue:.5f}\")\n",
    "\n",
    "    # Check significance after Bonferroni correction\n",
    "    if result.pvalue < bonferroni_corrected_alpha:\n",
    "         print(\"Significant difference after Bonferroni correction.\\n\")\n",
    "    else:\n",
    "         print(\"No significant difference after Bonferroni correction.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562f3f3-155b-4fd0-8408-fa63adcccd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Weighted precision, recall, f1-score evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6b6f4-0aed-4d31-a3c5-85075895e974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4d5d7-dd7d-4e7f-a9e9-389e5d352d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_performance_eval(pred_labels, exp_name):\n",
    "    columns = ['Degree of PD', 'Weighted Precision', 'Weighted Recall', 'Weighted F1-Score']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.loc[0, 'Degree of PD'] = exp_name\n",
    "    df.loc[0, 'Weighted Precision'] = precision_score(gold_labels, pred_labels, average = 'weighted')\n",
    "    df.loc[0, 'Weighted Recall'] = recall_score(gold_labels, pred_labels, average = 'weighted')\n",
    "    df.loc[0, 'Weighted F1-Score'] = f1_score(gold_labels, pred_labels, average = 'weighted')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd252a-7496-498b-afeb-5044903b924c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "none_w_perf_eval = weighted_performance_eval(none_pd_pred, 'None')\n",
    "low_w_perf_eval = weighted_performance_eval(low_pd_pred, 'Low')\n",
    "high_w_perf_eval = weighted_performance_eval(high_pd_pred, 'High')\n",
    "high_RBP_w_perf_eval = weighted_performance_eval(high_rbp_pd_pred, 'High with RBP')\n",
    "w_perf_eval = pd.concat([none_w_perf_eval, low_w_perf_eval, high_w_perf_eval, high_RBP_w_perf_eval])\n",
    "w_perf_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df5930-1563-464e-803c-4a1543afc046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for 10,000 bootstrap iterations to generate 95% confidence intervals\n",
    "# (for weighted precision, weighted recall, weighted f1-scores)\n",
    "\n",
    "def bootstrap_precision(gold_labels, pred_labels):\n",
    "\n",
    "    n_iterations = 10000  # Number of bootstrap resamples\n",
    "    n_samples = len(gold_labels)\n",
    "    bootstrapped_precisions = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)  # Resample indices\n",
    "        bootstrapped_precision = precision_score(gold_labels[indices], pred_labels[indices], average = 'weighted')\n",
    "        bootstrapped_precisions.append(bootstrapped_precision)\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    lower = np.percentile(bootstrapped_precisions, 2.5)\n",
    "    upper = np.percentile(bootstrapped_precisions, 97.5)\n",
    "    \n",
    "    return(round(lower, 4), round(upper, 4))\n",
    "\n",
    "def bootstrap_recall(gold_labels, pred_labels):\n",
    "\n",
    "    n_iterations = 10000  # Number of bootstrap resamples\n",
    "    n_samples = len(gold_labels)\n",
    "    bootstrapped_recalls = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)  # Resample indices\n",
    "        bootstrapped_recall = recall_score(gold_labels[indices], pred_labels[indices], average = 'weighted')\n",
    "        bootstrapped_recalls.append(bootstrapped_recall)\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    lower = np.percentile(bootstrapped_recalls, 2.5)\n",
    "    upper = np.percentile(bootstrapped_recalls, 97.5)\n",
    "    \n",
    "    return(round(lower, 4), round(upper, 4))\n",
    "\n",
    "def bootstrap_f1(gold_labels, pred_labels):\n",
    "\n",
    "    n_iterations = 10000  # Number of bootstrap resamples\n",
    "    n_samples = len(gold_labels)\n",
    "    bootstrapped_f1s = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)  # Resample indices\n",
    "        bootstrapped_f1 = f1_score(gold_labels[indices], pred_labels[indices], average = 'weighted')\n",
    "        bootstrapped_f1s.append(bootstrapped_f1)\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    lower = np.percentile(bootstrapped_f1s, 2.5)\n",
    "    upper = np.percentile(bootstrapped_f1s, 97.5)\n",
    "    \n",
    "    return(round(lower, 4), round(upper, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cad614-fd1a-4073-be1e-c4a4b410fb8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Individual precision, recall, f1-score evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729568c-4fce-42cf-a6c2-ed838cf47209",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example is provided with respect to volume classification experiments - high volume (HV) and low volume (LV) individual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb066f0-54e4-46c2-850f-36221ac597c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def performance_eval(pred_labels, exp_name):\n",
    "    \n",
    "    columns = ['Degree of PD', 'HV precision', 'LV precision', 'HV recall', 'LV recall', 'HV F1', 'LV F1', ]\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    df.loc[0, 'Degree of PD'] = exp_name\n",
    "    \n",
    "    df.loc[0, 'HV precision'] = round(precision_score(gold_labels, pred_labels, pos_label = 'H'), 3)\n",
    "    df.loc[0, 'LV precision'] = round(precision_score(gold_labels, pred_labels, pos_label = 'L'), 3)\n",
    "    \n",
    "    df.loc[0, 'HV recall'] = round(recall_score(gold_labels, pred_labels, pos_label = 'H'), 3)\n",
    "    df.loc[0, 'LV recall'] = round(recall_score(gold_labels, pred_labels, pos_label = 'L'), 3)\n",
    "    \n",
    "    df.loc[0, 'HV F1'] = round(f1_score(gold_labels, pred_labels, pos_label = 'H'), 3)\n",
    "    df.loc[0, 'LV F1'] = round(f1_score(gold_labels, pred_labels, pos_label = 'L'), 3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153db01d-e499-4655-b4c6-85a202c9edb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "none_perf_eval = performance_eval(none_pd_pred, 'None')\n",
    "low_perf_eval = performance_eval(low_pd_pred, 'Low')\n",
    "high_perf_eval = performance_eval(high_pd_pred, 'High')\n",
    "high_rbp_perf_eval = performance_eval(high_rbp_pd_pred, 'High with RBP')\n",
    "individual_perf_eval = pd.concat([none_perf_eval, low_perf_eval, high_perf_eval, high_rbp_perf_eval])\n",
    "individual_perf_eval"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
